diff --git a/hadoop-hdfs-project/hadoop-hdfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
index 55b8c6f..edc9723 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
@@ -35,6 +35,7 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
     <is.hadoop.component>true</is.hadoop.component>
     <require.fuse>false</require.fuse>
     <require.libwebhdfs>false</require.libwebhdfs>
+    <cit.energy.version>1.3-SNAPSHOT</cit.energy.version>
   </properties>
 
   <dependencies>
@@ -180,6 +181,11 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
       <artifactId>netty</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>de.tuberlin.cit.project.energy</groupId>
+      <artifactId>cit-energy-project-hadoop</artifactId>
+      <version>${cit.energy.version}</version>
+    </dependency>
   </dependencies>
 
   <build>
@@ -550,6 +556,22 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
           </excludes>
         </configuration>
       </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <executions>
+          <execution>
+            <phase>package</phase>
+            <goals>
+              <goal>jar</goal>
+            </goals>
+            <configuration>
+              <finalName>${project.artifactId}-${project.version}-cit-energy-project-${cit.energy.version}</finalName>
+              <archive><compress/></archive>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
     </plugins>
   </build>
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
index 2919566..3139e78 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
@@ -633,4 +633,16 @@
   public static final int DEFAULT_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE = 0;
   public static final String DFS_NFS_KEYTAB_FILE_KEY = "dfs.nfs.keytab.file";
   public static final String DFS_NFS_USER_NAME_KEY = "dfs.nfs.kerberos.principal";
+
+  // Energy project
+  public static final String DFS_ENERGY_ZABBIX_HOSTNAME = "dfs.energy.zabbix.hostname";
+  public static final String DFS_ENERGY_ZABBIX_HOSTNAME_DEFAULT = "mpjss14.cit.tu-berlin.de";
+  public static final String DFS_ENERGY_ZABBIX_PORT = "dfs.energy.zabbix.port";
+  public static final int DFS_ENERGY_ZABBIX_PORT_DEFAULT = 10051;
+  public static final String DFS_ENERGY_ZABBIX_REST_URL = "dfs.energy.zabbix.rest.url";
+  public static final String DFS_ENERGY_ZABBIX_REST_URL_DEFAULT = "https://mpjss14.cit.tu-berlin.de/zabbix/api_jsonrpc.php";
+  public static final String DFS_ENERGY_ZABBIX_REST_USERNAME = "dfs.energy.zabbix.rest.username";
+  public static final String DFS_ENERGY_ZABBIX_REST_USERNAME_DEFAULT = "admin";
+  public static final String DFS_ENERGY_ZABBIX_REST_PASSWORD = "dfs.energy.zabbix.rest.password";
+  public static final String DFS_ENERGY_ZABBIX_REST_PASSWORD_DEFAULT = "zabbix";
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DNConf.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DNConf.java
index d72c3b6..f91c08b 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DNConf.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DNConf.java
@@ -90,6 +90,9 @@
 
   final long maxLockedMemory;
 
+  final String zabbixHostname;
+  final int zabbixPort;
+
   public DNConf(Configuration conf) {
     socketTimeout = conf.getInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY,
         HdfsServerConstants.READ_TIMEOUT);
@@ -167,6 +170,13 @@ public DNConf(Configuration conf) {
     this.restartReplicaExpiry = conf.getLong(
         DFS_DATANODE_RESTART_REPLICA_EXPIRY_KEY,
         DFS_DATANODE_RESTART_REPLICA_EXPIRY_DEFAULT) * 1000L;
+
+    this.zabbixHostname = conf.get(
+        DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME,
+        DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME_DEFAULT);
+    this.zabbixPort = conf.getInt(
+        DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT,
+        DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT_DEFAULT);
   }
   
   // We get minimumNameNodeVersion via a method so it can be mocked out in tests.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
index f390795..13a91f3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
@@ -45,6 +45,7 @@
 import java.util.Arrays;
 
 import org.apache.commons.logging.Log;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.ExtendedBlockId;
 import org.apache.hadoop.hdfs.ShortCircuitShm.SlotId;
 import org.apache.hadoop.hdfs.net.Peer;
@@ -86,6 +87,7 @@
 import com.google.common.net.InetAddresses;
 import com.google.protobuf.ByteString;
 
+import de.tuberlin.cit.project.energy.hadoop.DataNodeTransferObserver;
 
 /**
  * Thread for processing incoming/outgoing data stream.
@@ -104,6 +106,7 @@
   private long opStartTime; //the start time of receiving an Op
   private final InputStream socketIn;
   private OutputStream socketOut;
+  private final DataNodeTransferObserver dataNodeTransferObserver;
 
   /**
    * Client Name used in previous operation. Not available on first request
@@ -133,6 +136,9 @@ private DataXceiver(Peer peer, DataNode datanode,
       LOG.debug("Number of active connections is: "
           + datanode.getXceiverCount());
     }
+
+    this.dataNodeTransferObserver = new DataNodeTransferObserver(
+        this.dnConf.zabbixHostname, this.dnConf.zabbixPort);
   }
 
   /**
@@ -475,6 +481,8 @@ public void readBlock(final ExtendedBlock block,
     checkAccess(out, true, block, blockToken,
         Op.READ_BLOCK, BlockTokenSecretManager.AccessMode.READ);
   
+    this.dataNodeTransferObserver.readBlockStart(datanode.getDisplayName(), remoteAddress, blockToken, blockOffset, length);
+
     // send the block
     BlockSender blockSender = null;
     DatanodeRegistration dnR = 
@@ -526,6 +534,7 @@ public void readBlock(final ExtendedBlock block,
       }
       datanode.metrics.incrBytesRead((int) read);
       datanode.metrics.incrBlocksRead();
+      this.dataNodeTransferObserver.readBlockEnd(datanode.getDisplayName(), remoteAddress, blockToken, blockOffset, length, read, elapsed());
     } catch ( SocketException ignored ) {
       if (LOG.isTraceEnabled()) {
         LOG.trace(dnR + ":Ignoring exception while serving " + block + " to " +
@@ -606,6 +615,12 @@ public void writeBlock(final ExtendedBlock block,
     checkAccess(replyOut, isClient, block, blockToken,
         Op.WRITE_BLOCK, BlockTokenSecretManager.AccessMode.WRITE);
 
+    this.dataNodeTransferObserver.writeBlockStart(
+        datanode.getDisplayName(), remoteAddress,
+        isDatanode, blockToken,
+        pipelineSize, targets,
+        minBytesRcvd, maxBytesRcvd);
+
     DataOutputStream mirrorOut = null;  // stream to next target
     DataInputStream mirrorIn = null;    // reply from next target
     Socket mirrorSock = null;           // socket to next target
@@ -754,6 +769,8 @@ public void writeBlock(final ExtendedBlock block,
         block.setGenerationStamp(latestGenerationStamp);
         block.setNumBytes(minBytesRcvd);
       }
+
+      this.dataNodeTransferObserver.writeBlockEnd(datanode.getDisplayName(), remoteAddress, isDatanode, blockToken, block, pipelineSize, elapsed());
       
       // if this write is for a replication request or recovering
       // a failed close for client, then confirm block. For other client-writes,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
index 82c44aa..e646eb2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
@@ -124,6 +124,7 @@
 import org.apache.hadoop.hdfs.server.protocol.StorageBlockReport;
 import org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks;
 import org.apache.hadoop.hdfs.server.protocol.StorageReport;
+import org.apache.hadoop.hdfs.web.JsonUtil;
 import org.apache.hadoop.io.EnumSetWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.ipc.ProtobufRpcEngine;
@@ -156,6 +157,9 @@
 import com.google.common.annotations.VisibleForTesting;
 import com.google.protobuf.BlockingService;
 
+import de.tuberlin.cit.project.energy.hadoop.BlockObserver;
+import de.tuberlin.cit.project.energy.hadoop.EnergyConservingDataNodeFilter;
+
 /**
  * This class is responsible for handling all of the RPC calls to the NameNode.
  * It is created, started, and stopped by {@link NameNode}.
@@ -183,6 +187,9 @@
   
   private final String minimumDataNodeVersion;
 
+  private final BlockObserver blockObserver;
+  private final EnergyConservingDataNodeFilter energyConservingDataNodeFilter;
+
   public NameNodeRpcServer(Configuration conf, NameNode nn)
       throws IOException {
     this.nn = nn;
@@ -357,6 +364,29 @@ public NameNodeRpcServer(Configuration conf, NameNode nn)
         AclException.class,
         FSLimitException.PathComponentTooLongException.class,
         FSLimitException.MaxDirectoryItemsExceededException.class);
+
+    this.blockObserver = new BlockObserver(this,
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME_DEFAULT),
+      conf.getInt(DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_URL,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_URL_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_USERNAME,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_USERNAME_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_PASSWORD,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_PASSWORD_DEFAULT));
+    this.energyConservingDataNodeFilter = new EnergyConservingDataNodeFilter(
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_HOSTNAME_DEFAULT),
+      conf.getInt(DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_PORT_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_URL,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_URL_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_USERNAME,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_USERNAME_DEFAULT),
+      conf.get(DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_PASSWORD,
+          DFSConfigKeys.DFS_ENERGY_ZABBIX_REST_PASSWORD_DEFAULT));
  }
 
   /** Allow access to the client RPC server for testing */
@@ -494,8 +524,14 @@ public LocatedBlocks getBlockLocations(String src,
                                           long length) 
       throws IOException {
     metrics.incrGetBlockLocations();
-    return namesystem.getBlockLocations(getClientMachine(), 
-                                        src, offset, length);
+
+    LocatedBlocks locatedBlocks = namesystem.getBlockLocations(
+        getClientMachine(), src, offset, length);
+
+    locatedBlocks = this.energyConservingDataNodeFilter.filterBlockLocations(
+        locatedBlocks, src, getCurrentUserName(), getClientMachine());
+
+    return locatedBlocks;
   }
   
   @Override // ClientProtocol
@@ -547,8 +583,18 @@ public boolean recoverLease(String src, String clientName) throws IOException {
 
   @Override // ClientProtocol
   public boolean setReplication(String src, short replication) 
-    throws IOException {  
-    return namesystem.setReplication(src, replication);
+    throws IOException {
+
+    LocatedBlocks blocksBefore = namesystem.getBlockLocations(getClientMachine(), src, 0, Long.MAX_VALUE);
+    String blocksBeforeJson = JsonUtil.toJsonString(blocksBefore); // freeze current list as JSON
+
+    boolean result = namesystem.setReplication(src, replication);
+
+    if (result) {
+      this.blockObserver.setReplication(src, getCurrentUserName(), replication, blocksBeforeJson);
+    }
+
+    return result;
   }
     
   @Override // ClientProtocol
@@ -638,7 +684,15 @@ public boolean complete(String src, String clientName,
       stateChangeLog.debug("*DIR* NameNode.complete: "
           + src + " fileId=" + fileId +" for " + clientName);
     }
-    return namesystem.completeFile(src, clientName, last, fileId);
+
+    LocatedBlocks locatedBlocks = namesystem.getBlockLocations(getClientMachine(), src, 0, Long.MAX_VALUE);
+    boolean result = namesystem.completeFile(src, clientName, last, fileId);
+
+    if (result) {
+	    this.blockObserver.complete(src, fileId, getCurrentUserName(), locatedBlocks, result);
+    }
+
+    return result;
   }
 
   /**
@@ -701,6 +755,11 @@ public boolean rename(String src, String dst) throws IOException {
   
   @Override // ClientProtocol
   public void concat(String trg, String[] src) throws IOException {
+    LocatedBlocks srcBlocks[] = new LocatedBlocks[src.length];
+    for (int i = 0; i < src.length; i++)
+      srcBlocks[i] = namesystem.getBlockLocations(getClientMachine(), src[i], 0, Long.MAX_VALUE);
+    this.blockObserver.concat(trg, src, getCurrentUserName(), srcBlocks);
+
     namesystem.concat(trg, src);
   }
   
@@ -724,9 +783,16 @@ public boolean delete(String src, boolean recursive) throws IOException {
       stateChangeLog.debug("*DIR* Namenode.delete: src=" + src
           + ", recursive=" + recursive);
     }
+
+    LocatedBlocks removedBlocks = namesystem.getBlockLocations(getClientMachine(), src, 0, Long.MAX_VALUE);
+    String removedBlocksJson = JsonUtil.toJsonString(removedBlocks); // freeze current list as JSON
+
     boolean ret = namesystem.delete(src, recursive);
-    if (ret) 
+
+    if (ret) {
+      this.blockObserver.delete(src, getCurrentUserName(), removedBlocksJson, recursive);
       metrics.incrDeleteFileOps();
+    }
     return ret;
   }
 
@@ -1225,6 +1291,15 @@ private static String getClientMachine() {
     return clientMachine;
   }
 
+  private static String getCurrentUserName() throws IOException {
+    if (NamenodeWebHdfsMethods.isWebHdfsInvocation())
+      return UserGroupInformation.getCurrentUser().getUserName();
+    else if(Server.isRpcInvocation())
+      return Server.getRemoteUser().getUserName();
+    else
+      return null;
+  }
+
   @Override
   public DataEncryptionKey getDataEncryptionKey() throws IOException {
     return namesystem.getBlockManager().generateDataEncryptionKey();
